digraph reference_template {
    graph [
        goal="$goal",
        rankdir=LR,
        default_max_retry=3,
        retry_target="implement",
        fallback_retry_target="debate_consolidate",
        provenance_version="1",
        model_stylesheet="
            * { llm_model: DEFAULT_MODEL; llm_provider: DEFAULT_PROVIDER; }
            .hard { llm_model: HARD_MODEL; llm_provider: HARD_PROVIDER; }
            .verify { llm_model: VERIFY_MODEL; llm_provider: VERIFY_PROVIDER; }
            .branch-a { llm_model: BRANCH_A_MODEL; llm_provider: BRANCH_A_PROVIDER; }
            .branch-b { llm_model: BRANCH_B_MODEL; llm_provider: BRANCH_B_PROVIDER; }
            .branch-c { llm_model: BRANCH_C_MODEL; llm_provider: BRANCH_C_PROVIDER; }
        "
    ]

    // NOTE: Prompts in this template are abbreviated structural stubs.
    // Generated pipelines must use the full status-file contract from
    // Phase 4 (fallback path + nested-directory warning).
    //
    // Status JSON schema note:
    // - Prefer the canonical metaspec schema: {"status":"success|fail|retry|..."}.
    // - The engine still accepts legacy {"outcome":"..."} shapes, but templates should
    //   steer agents/tools toward canonical "status".
    exit  [shape=Msquare, label="Exit"]

    subgraph cluster_bootstrap {
        label="Bootstrap"
        start [shape=Mdiamond, label="Start"]

        // Toolchain gate - fail fast before LLM stages
        check_toolchain [
            shape=parallelogram,
            max_retries=0,
            tool_command="echo 'Replace with project-specific toolchain checks. Enumerate every tool that will be required and check it thoroughly; we strongly prefer a fast failure.'; exit 0"
        ]

        // Spec expansion - bootstraps .ai/spec.md from vague input. If the input is already a fully detailed specification, this node may be omitted. Only node allowed to use auto_status=true (skip verify).
        expand_spec [
            shape=box,
            auto_status=true,
            prompt="Goal: $goal\n\nCreate or reuse a canonical spec at .ai/spec.md.\n\nStep 1 (reuse first): If .ai/spec.md already exists and is a real spec (not a placeholder), keep its meaning. You may do form-only cleanup (headings/whitespace) if needed.\n\nStep 2 (reuse repo spec): If the repo already contains an adequate spec/requirements doc (for example under docs/, a requirements/spec file, or a detailed README section), prefer reusing it. If downstream nodes expect .ai/spec.md, copy that content verbatim into .ai/spec.md (form-only formatting allowed).\n\nStep 3 (expand): Otherwise, expand $goal into a spec with sections:\n- Overview\n- Scope (in/out)\n- Constraints\n- Assumptions\n- Acceptance Criteria\n- Verification\n- Non-goals/Deferrals\n\nWrite the result to .ai/spec.md."
        ]

        // DoD routing - skip DoD generation on loop iterations
        check_dod [
            shape=box,
            label="DoD exists?",
            prompt="Goal: $goal\n\nCheck whether .ai/definition_of_done.md exists AND is a real project Definition of Done (not a placeholder).\n\nDoD rubric (must be specific, verifiable, and not over-prescriptive):\n- Scope: defines what is in-scope and what is out-of-scope.\n- Deliverables: names concrete outputs/outcomes (artifacts, behaviors, docs), not implementation steps.\n- Acceptance criteria: includes observable pass/fail criteria a reviewer can verify.\n- Verification: includes how to verify (commands or steps) appropriate for the project, without forcing a particular tool unless required.\n- Quality/safety: includes project-appropriate quality expectations stated as outcomes/evidence.\n- Non-goals/deferrals: explicitly calls out what is intentionally not being done in this iteration.\n\nCoverage checklist: the DoD must explicitly address each item (either include criteria, or mark N/A with a short reason):\n- Build/package/install correctness (or equivalent)\n- Automated tests (or explicit manual verification steps)\n- Lint/format/static analysis (or N/A)\n- Documentation/user guidance (or N/A)\n- Compatibility/breaking changes/migrations (or N/A)\n- Security/privacy considerations (or N/A)\n- Operational readiness when relevant (logging, error handling, observability, rollback safety)\n- Performance/reliability when relevant\n\nClassify:\n- Set `status` to `has_dod` if the file meets the rubric AND covers the checklist.\n- Set `status` to `needs_dod` otherwise.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nDo not write nested status.json files after cd.\n\nNote: routing edges use condition=\"outcome=<status>\" (outcome mirrors the status.json `status` value)."
        ]
    }

    subgraph cluster_dod {
        label="DoD Fanout"
        node [shape=box]

        // Explicit fan-out
        dod_fanout [shape=component, label="DoD Fan-Out"]

        // DoD fan-out: 3 branches, each a DIFFERENT provider via class
        dod_a [
            class="branch-a",
            prompt="Goal: $goal\n\nPropose a project Definition of Done (DoD). Read .ai/spec.md.\n\nRequirements:\n- DoD must be a checklist of outcomes/evidence, not a plan.\n- Each item must be verifiable (someone can check it and say pass/fail).\n- Avoid prescribing the implementation approach unless the spec explicitly requires it.\n- Include scope, deliverables, acceptance criteria, verification approach, and explicit non-goals/deferrals.\n- Keep it general enough for the project type while still preventing obvious 'done-but-not-really' loopholes.\n\nCoverage checklist: explicitly address each item (include criteria, or mark N/A with a short reason):\n- Build/package/install correctness (or equivalent)\n- Automated tests (or explicit manual verification steps)\n- Lint/format/static analysis (or N/A)\n- Documentation/user guidance (or N/A)\n- Compatibility/breaking changes/migrations (or N/A)\n- Security/privacy considerations (or N/A)\n- Operational readiness when relevant (logging, error handling, observability, rollback safety)\n- Performance/reliability when relevant\n\nWrite to .ai/dod_a.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
        dod_b [
            class="branch-b",
            prompt="Goal: $goal\n\nPropose a project Definition of Done (DoD). Read .ai/spec.md.\n\nRequirements:\n- DoD must be a checklist of outcomes/evidence, not a plan.\n- Each item must be verifiable (someone can check it and say pass/fail).\n- Avoid prescribing the implementation approach unless the spec explicitly requires it.\n- Include scope, deliverables, acceptance criteria, verification approach, and explicit non-goals/deferrals.\n- Keep it general enough for the project type while still preventing obvious 'done-but-not-really' loopholes.\n\nCoverage checklist: explicitly address each item (include criteria, or mark N/A with a short reason):\n- Build/package/install correctness (or equivalent)\n- Automated tests (or explicit manual verification steps)\n- Lint/format/static analysis (or N/A)\n- Documentation/user guidance (or N/A)\n- Compatibility/breaking changes/migrations (or N/A)\n- Security/privacy considerations (or N/A)\n- Operational readiness when relevant (logging, error handling, observability, rollback safety)\n- Performance/reliability when relevant\n\nWrite to .ai/dod_b.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
        dod_c [
            class="branch-c",
            prompt="Goal: $goal\n\nPropose a project Definition of Done (DoD). Read .ai/spec.md.\n\nRequirements:\n- DoD must be a checklist of outcomes/evidence, not a plan.\n- Each item must be verifiable (someone can check it and say pass/fail).\n- Avoid prescribing the implementation approach unless the spec explicitly requires it.\n- Include scope, deliverables, acceptance criteria, verification approach, and explicit non-goals/deferrals.\n- Keep it general enough for the project type while still preventing obvious 'done-but-not-really' loopholes.\n\nCoverage checklist: explicitly address each item (include criteria, or mark N/A with a short reason):\n- Build/package/install correctness (or equivalent)\n- Automated tests (or explicit manual verification steps)\n- Lint/format/static analysis (or N/A)\n- Documentation/user guidance (or N/A)\n- Compatibility/breaking changes/migrations (or N/A)\n- Security/privacy considerations (or N/A)\n- Operational readiness when relevant (logging, error handling, observability, rollback safety)\n- Performance/reliability when relevant\n\nWrite to .ai/dod_c.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
        consolidate_dod [
            prompt="Goal: $goal\n\nSynthesize the DoD proposals into a single consensus DoD. Read .ai/spec.md.\n\nImportant: The DoD branches run in isolated worktrees when triggered via dod_fanout (shape=component).\nTo gather inputs:\n1) Read $KILROY_LOGS_ROOT/dod_fanout/parallel_results.json.\n2) From each branch worktree_dir, read any of:\n- .ai/dod_a.md\n- .ai/dod_b.md\n- .ai/dod_c.md\nIf parallel_results.json is missing, fall back to reading those files from the current worktree.\n\nOutput requirements:\n- Produce a single DoD that meets the DoD rubric (scope, deliverables, acceptance criteria, verification, quality/safety outcomes, non-goals).\n- Coverage checklist: explicitly address each item (include criteria, or mark N/A with a short reason):\n  - Build/package/install correctness (or equivalent)\n  - Automated tests (or explicit manual verification steps)\n  - Lint/format/static analysis (or N/A)\n  - Documentation/user guidance (or N/A)\n  - Compatibility/breaking changes/migrations (or N/A)\n  - Security/privacy considerations (or N/A)\n  - Operational readiness when relevant (logging, error handling, observability, rollback safety)\n  - Performance/reliability when relevant\n- Prefer 'what' and 'evidence' over 'how'.\n- Remove contradictions; keep it realistically checkable.\n\nWrite the final DoD to .ai/definition_of_done.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
    }

    subgraph cluster_planning {
        label="Planning Fanout"
        node [shape=box]

        // Explicit fan-out
        plan_fanout [shape=component, label="Plan Fan-Out"]

        // Planning fan-out: 3 branches with different providers
        // Each planner reads postmortem (if exists) to incorporate lessons.
        plan_a [
            class="branch-a",
            prompt="Goal: $goal\n\nCreate an implementation plan.\nRead .ai/spec.md, .ai/definition_of_done.md.\nIf .ai/postmortem_latest.md exists, incorporate its lessons.\nWrite to .ai/plan_a.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
        plan_b [
            class="branch-b",
            prompt="Goal: $goal\n\nCreate an implementation plan.\nRead .ai/spec.md, .ai/definition_of_done.md.\nIf .ai/postmortem_latest.md exists, incorporate its lessons.\nWrite to .ai/plan_b.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
        plan_c [
            class="branch-c",
            prompt="Goal: $goal\n\nCreate an implementation plan.\nRead .ai/spec.md, .ai/definition_of_done.md.\nIf .ai/postmortem_latest.md exists, incorporate its lessons.\nWrite to .ai/plan_c.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
        debate_consolidate [
            prompt="Goal: $goal\n\nSynthesize the planning branches into a best-of-breed final plan.\n\nImportant: The planning branches run in isolated worktrees when triggered via plan_fanout (shape=component).\nTo gather inputs:\n1) Read $KILROY_LOGS_ROOT/plan_fanout/parallel_results.json.\n2) From each branch worktree_dir, read any of:\n- .ai/plan_a.md\n- .ai/plan_b.md\n- .ai/plan_c.md\nIf parallel_results.json is missing, fall back to reading those files from the current worktree.\n\nResolve conflicts. Ensure dependency order.\nIf .ai/postmortem_latest.md exists, verify the plan addresses every issue.\nWrite to .ai/plan_final.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
    }

    subgraph cluster_implement_verify {
        label="Implement And Verify"

        // Single-writer implementation - the only code-writing node
        implement [
            shape=box,
            class="hard",
            max_retries=2,
            prompt="Goal: $goal\n\nExecute .ai/plan_final.md. Read .ai/definition_of_done.md for acceptance criteria.\nIf .ai/postmortem_latest.md exists, prioritize fixing those issues.\n\nUse progressive compilation: get each module compiling before starting the next.\nLog progress to .ai/implementation_log.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\n\nIf `status` is `fail`, include failure_reason, details, failure_class, and failure_signature.\n\nSet `status` to `success` if build passes; otherwise set `status` to `fail`."
        ]
        check_implement [shape=diamond, label="Implement OK?"]

        // Auto-fix formatting before verification. Replace placeholder with
        // the project's canonical formatter (e.g. gofmt -w ., cargo fmt,
        // black ., prettier --write .). Runs unconditionally so verify_fmt
        // is a pure assertion.
        fix_fmt [
            shape=parallelogram,
            max_retries=0,
            tool_command="echo 'Replace with project-specific auto-formatter (e.g. gofmt -w ., cargo fmt, black ., prettier --write .)'; exit 0"
        ]

        // Deterministic verification gates - tool nodes first, then semantic review
        // max_retries=0: format checks are deterministic â€” re-running on unchanged
        // code always produces the same result. Route to postmortem immediately
        // rather than wasting retries.
        verify_fmt [
            shape=parallelogram,
            max_retries=0,
            tool_command="echo 'Replace with project-specific formatter check'; exit 0"
        ]
        check_fmt [shape=diamond, label="Fmt OK?"]

        verify_build [
            shape=parallelogram,
            tool_command="echo 'Replace with project-specific build check'; exit 0"
        ]
        check_build [shape=diamond, label="Build OK?"]

        verify_test [
            shape=parallelogram,
            tool_command="echo 'Replace with project-specific test check'; exit 0"
        ]
        check_test [shape=diamond, label="Tests OK?"]

        // max_retries=0: artifact hygiene checks are deterministic.
        verify_artifacts [
            shape=parallelogram,
            max_retries=0,
            tool_command="echo 'Replace with artifact hygiene check (include both .cargo-target* and .cargo_target* patterns)'; exit 0"
        ]
        check_artifacts [shape=diamond, label="Artifacts OK?"]

        verify_fidelity [
            shape=box,
            class="verify",
            prompt="Perform semantic fidelity review after deterministic checks pass.\n\nRead implementation outputs and verify behavior matches .ai/definition_of_done.md and .ai/spec.md.\nIf semantic gaps exist, set `status` to `fail` with a stable failure_reason code (for example semantic_fidelity_gap) and include details.\n\nWrite results to .ai/verify_fidelity.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\n\nIf `status` is `fail`, include failure_reason and details.\n\nSet `status` to `success` if semantic review passes; otherwise set `status` to `fail`."
        ]
        check_impl [shape=diamond, label="Impl OK?"]
    }

    subgraph cluster_review {
        label="Review Fanout"
        node [shape=box]

        // Explicit fan-out; convergence remains edge-based at review_consensus.
        review_fanout [shape=component, label="Review Fan-Out"]

        // Review fan-out: 3 branches with different providers
        review_a [
            class="branch-a",
            prompt="Goal: $goal\n\nReview the implementation against .ai/definition_of_done.md.\nCheck build, completeness, correctness, tests.\nWrite review to .ai/review_a.md with APPROVED or REJECTED verdict.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\n\nIf `status` is `fail`, include failure_reason and details listing specific gaps.\n\nSet `status` to `success` if the implementation meets the DoD; otherwise set `status` to `fail`."
        ]
        review_b [
            class="branch-b",
            prompt="Goal: $goal\n\nReview the implementation against .ai/definition_of_done.md.\nCheck build, completeness, correctness, tests.\nWrite review to .ai/review_b.md with APPROVED or REJECTED verdict.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\n\nIf `status` is `fail`, include failure_reason and details listing specific gaps.\n\nSet `status` to `success` if the implementation meets the DoD; otherwise set `status` to `fail`."
        ]
        review_c [
            class="branch-c",
            prompt="Goal: $goal\n\nReview the implementation against .ai/definition_of_done.md.\nCheck build, completeness, correctness, tests.\nWrite review to .ai/review_c.md with APPROVED or REJECTED verdict.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\n\nIf `status` is `fail`, include failure_reason and details listing specific gaps.\n\nSet `status` to `success` if the implementation meets the DoD; otherwise set `status` to `fail`."
        ]
        review_consensus [
            goal_gate=true,
            prompt="Goal: $goal\n\nSynthesize the review branches into a single consensus.\nRead .ai/definition_of_done.md for criteria.\n\nImportant: The review branches run in isolated worktrees when triggered via review_fanout (shape=component).\nTo gather inputs:\n1) Read $KILROY_LOGS_ROOT/review_fanout/parallel_results.json.\n2) From each branch worktree_dir, read any of:\n- .ai/review_a.md\n- .ai/review_b.md\n- .ai/review_c.md\nIf parallel_results.json is missing, fall back to reading those files from the current worktree.\n\nConsensus policy:\n- If 2+ reviewers APPROVED and no critical gaps: set `status` to `success`.\n- Otherwise: set `status` to `retry` with failure_reason and details listing specific issues.\n\nWrite consensus to .ai/review_consensus.md.\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\n\nIf `status` is `retry`, include failure_reason and details.\n\nSet `status` to `success` or `retry`."
        ]
    }

    subgraph cluster_postmortem {
        label="Postmortem"
        node [shape=box]

        // Postmortem - analyze failure, guide next iteration
        postmortem [
            prompt="Goal: $goal\n\nAnalyze why the implementation failed.\nRead .ai/review_consensus.md.\n\nImportant: The review branches run in isolated worktrees when triggered via review_fanout (shape=component).\nTo gather review inputs:\n1) Read $KILROY_LOGS_ROOT/review_fanout/parallel_results.json.\n2) From each branch worktree_dir, read any of:\n- .ai/review_a.md\n- .ai/review_b.md\n- .ai/review_c.md\nIf parallel_results.json is missing, fall back to reading those files from the current worktree.\n\nRead .ai/implementation_log.md.\n\nProduce actionable guidance: root causes, what worked, what failed, specific fixes.\nThe next iteration must NOT start from scratch - preserve working code and fix gaps.\n\nWrite to .ai/postmortem_latest.md (overwrite previous).\n\nWrite canonical status JSON per the engine-provided \"Execution status contract\" preamble.\nSet `status` to `success`."
        ]
    }

    // =========================================================================
    // Flow
    // =========================================================================

    // Linear start: toolchain gate -> spec -> DoD check
    start -> check_toolchain
    check_toolchain -> expand_spec [condition="outcome=success"]
    check_toolchain -> check_toolchain [condition="outcome=fail && context.failure_class=transient_infra", loop_restart=true]
    check_toolchain -> postmortem [condition="outcome=fail && context.failure_class!=transient_infra"]
    check_toolchain -> postmortem
    expand_spec -> check_dod

    // DoD fan-out (if needed)
    check_dod -> dod_fanout [condition="outcome=needs_dod"]
    check_dod -> dod_fanout
    dod_fanout -> dod_a
    dod_fanout -> dod_b
    dod_fanout -> dod_c
    dod_a -> consolidate_dod
    dod_b -> consolidate_dod
    dod_c -> consolidate_dod
    consolidate_dod -> plan_fanout

    // Skip to planning if DoD exists
    check_dod -> plan_fanout [condition="outcome=has_dod"]

    // Planning fan-in -> debate -> implement
    plan_fanout -> plan_a
    plan_fanout -> plan_b
    plan_fanout -> plan_c
    plan_a -> debate_consolidate
    plan_b -> debate_consolidate
    plan_c -> debate_consolidate
    debate_consolidate -> implement

    // Verify/check inner loop (tool gates first, semantic review last)
    implement -> check_implement
    check_implement -> fix_fmt [condition="outcome=success"]
    fix_fmt -> verify_fmt
    check_implement -> implement  [condition="outcome=fail && context.failure_class=transient_infra", loop_restart=true]
    check_implement -> postmortem [condition="outcome=fail && context.failure_class!=transient_infra"]
    check_implement -> postmortem
    verify_fmt -> check_fmt
    check_fmt -> verify_build [condition="outcome=success"]
    check_fmt -> implement    [condition="outcome=fail && context.failure_class=transient_infra", loop_restart=true]
    check_fmt -> postmortem   [condition="outcome=fail && context.failure_class!=transient_infra"]
    check_fmt -> postmortem

    verify_build -> check_build
    check_build -> verify_test  [condition="outcome=success"]
    check_build -> implement    [condition="outcome=fail && context.failure_class=transient_infra", loop_restart=true]
    check_build -> postmortem   [condition="outcome=fail && context.failure_class!=transient_infra"]
    check_build -> postmortem

    verify_test -> check_test
    check_test -> verify_artifacts [condition="outcome=success"]
    check_test -> implement        [condition="outcome=fail && context.failure_class=transient_infra", loop_restart=true]
    check_test -> postmortem       [condition="outcome=fail && context.failure_class!=transient_infra"]
    check_test -> postmortem

    verify_artifacts -> check_artifacts
    check_artifacts -> verify_fidelity [condition="outcome=success"]
    check_artifacts -> implement       [condition="outcome=fail && context.failure_class=transient_infra", loop_restart=true]
    check_artifacts -> postmortem      [condition="outcome=fail && context.failure_class!=transient_infra"]
    check_artifacts -> postmortem

    verify_fidelity -> check_impl
    check_impl -> review_fanout [condition="outcome=success"]
    review_fanout -> review_a
    review_fanout -> review_b
    review_fanout -> review_c
    check_impl -> implement  [condition="outcome=fail && context.failure_class=transient_infra", loop_restart=true]
    check_impl -> postmortem [condition="outcome=fail && context.failure_class!=transient_infra"]
    check_impl -> postmortem

    // Review fan-in -> consensus
    review_a -> review_consensus
    review_b -> review_consensus
    review_c -> review_consensus

    // Consensus routing: success -> exit, anything else -> postmortem
    review_consensus -> exit [condition="outcome=success"]
    review_consensus -> postmortem

    // Hill-climbing loop: continue through prerequisite gate before re-planning
    postmortem -> check_toolchain
}
